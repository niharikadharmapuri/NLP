{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping using BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleURL = \"https://www.washingtonpost.com/news/the-switch/wp/2016/10/18/the-pentagons-massive-new-telescope-is-designed-to-track-space-junk-and-watch-out-for-killer-asteroids/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(articleURL)# make get request to access data from the API\n",
    "#print(type(page))\n",
    "#page.text # actual response text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #package\n",
    "result=requests.get(articleURL) # make get request to access data from the API\n",
    "#once you get a response from the API, you can access various properties of the result\n",
    "print(result.headers)\n",
    "print(result.status_code) \n",
    "print(result.text)# actual response text\n",
    "print(result.encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#display HTML string in the jupyter notebook\n",
    "from IPython.core.display import display, HTML \n",
    "display(HTML(page))\n",
    "#converting html_string into html object using HTML function \n",
    "#and then passing the HTML document to the display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"lxml\") #BS will convert html string to structured object that we\n",
    "# can query later. #others 'html.parser'\n",
    "#print(soup)# prints html document structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find('article') #any text corresponding to article is enclosed in 'article' tag\n",
    "#this prints text along with tag\n",
    "#finds only first article element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find('article').text# use text attribute to get the content of the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('article')# gives a list of articles elements that have tag article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "#join()returns a string in which the elements of seq have been joined by specified string separator\n",
    "#combing all the articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.encode('ascii', errors='replace').decode().replace(\"?\",\" \") #remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import *\n",
    "stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent = word_tokenize(text.lower())\n",
    "word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent=[word for word in word_sent if word not in _stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent #list of words in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict to store freq of each words in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq = FreqDist(word_sent)\n",
    "freq # dict stores words(that are not stopwords) as keys and their freq as values\n",
    "#basically a vocab of words in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "nlargest(10, freq, key=freq.get) #gives keys(words) that have top 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute significance score of each sentence\n",
    "### authors of an article tend to repeat words that are significant to the theme of the article\n",
    "### if a sentence has many such words, the sentence significance goes high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ranking stores score of each sentence\n",
    "ranking = defaultdict(int) #it doesnt throw error when we try to access keys not present in the dict \n",
    "                            #simply adds a new key to the dict\n",
    "\n",
    "# ranking dict will store sentences as keys and their importance as values\n",
    "for i,sent in enumerate(sents): #i iterates over indexes of sentences\n",
    "    for w in word_tokenize(sent.lower()): #for each word in a sentence\n",
    "        if w in freq:#if word is present in our vocab\n",
    "            ranking[i] += freq[w] # sentence score =sum of freq(in corpus) of the each word that appears in the sentence\n",
    "            \n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_idx = nlargest(4, ranking, key=ranking.get)# gives sentence index that have max score\n",
    "sents_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sents[j] for j in sorted(sents_idx)] #sort the sentences to make logical sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "def getTextWaPo(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,\"lxml\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "    return text.encode('ascii', errors='replace').decode().replace(\"?\",\" \")\n",
    "\n",
    "\n",
    "def summarize(text, n): # text and number of sentences to be included in our summary\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    assert n <= len(sents) #check if the text has required number of sentences\n",
    "    word_sent = word_tokenize(text.lower())\n",
    "    _stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "    \n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    \n",
    "    \n",
    "    ranking = defaultdict(int)\n",
    "    \n",
    "    for i,sent in enumerate(sents):\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in freq:\n",
    "                ranking[i] += freq[w]\n",
    "             \n",
    "        \n",
    "    sents_idx = nlargest(n, ranking, key=ranking.get)\n",
    "    return [sents[j] for j in sorted(sents_idx)]\n",
    "\n",
    "articleURL = \"https://www.washingtonpost.com/news/the-switch/wp/2016/10/18/the-pentagons-massive-new-telescope-is-designed-to-track-space-junk-and-watch-out-for-killer-asteroids/\"\n",
    "text = getTextWaPo(articleURL)\n",
    "summarize(text,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
